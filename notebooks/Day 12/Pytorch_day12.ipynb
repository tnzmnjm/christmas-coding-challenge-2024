{
 "cells": [
  {
   "cell_type": "code",
   "id": "6b7308edab8cc296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T10:51:09.086440Z",
     "start_time": "2024-12-12T10:51:09.081825Z"
    }
   },
   "source": [
    "import torch\n",
    "from urllib3.http2 import orig_HTTPSConnection\n",
    "\n",
    "\n",
    "# Creating a multilayer perceptron with two hidden layers\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "  def __init__(self, num_inputs, num_outputs):\n",
    "    super().__init__()\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        # First hidden layer\n",
    "        torch.nn.Linear(num_inputs, 30),\n",
    "        torch.nn.ReLU(),\n",
    "\n",
    "        # Second hidden layer\n",
    "        torch.nn.Linear(30, 20),\n",
    "        torch.nn.ReLU(),\n",
    "\n",
    "        # Output layer\n",
    "        torch.nn.Linear(20, num_outputs)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-12-12T10:51:12.411226Z",
     "start_time": "2024-12-12T10:51:12.407245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating a small toy dataset\n",
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "x_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],])\n",
    "\n",
    "y_test = torch.tensor([0, 1])"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "9694db86ec229f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T10:51:15.486849Z",
     "start_time": "2024-12-12T10:51:15.482243Z"
    }
   },
   "source": [
    "# Defining a custom Dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.features = X\n",
    "    self.labels = y\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    one_x = self.features[index]\n",
    "    one_y = self.labels[index]\n",
    "    return one_x, one_y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.labels.shape[0]\n",
    "\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(x_test, y_test)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "bdda07a1bb232fbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T10:51:17.626924Z",
     "start_time": "2024-12-12T10:51:17.621855Z"
    }
   },
   "source": [
    "# Instantiating data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "92c7c509c52a108b",
   "metadata": {},
   "source": [
    "### A.7 A typical training loop\n",
    "\n",
    "Let’s now train a neural network on the toy dataset. The following listing shows the training code."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T11:46:14.070633Z",
     "start_time": "2024-12-12T11:46:13.541598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Neural Network training in PyTorch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2) # The dataset has two features and two classes.\n",
    "optimiser = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.5  # The optimiser needs to know which parameters to optimise.\n",
    ")\n",
    "\n",
    "num_epochs = 3 #  There will be 3 complete passing through the entire training dataset.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # It's time to learn\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        logits = model(features) # The model makes its predictions based on the input features. These raw predictions (logits) haven't been converted to probabilities yet.\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        optimiser.zero_grad() # Sets the gradients from the previous round to 0 to prevent unintended gradient accumulation\n",
    "        loss.backward()    # Computes the gradients of the loss given the model parameters. This calculates how much each parameter in the model contributed to the error. Will calculate the gradients in the computation graph that PyTorch constructed in the background.\n",
    "        optimiser.step()   # The optimiser uses the gradients to update the model parameters making them slightly better for next time to minimise the loss. In the case of the SGD optimiser, this means multiplying the gradients with the learning rate and adding the scaled negative gradient to the parameters.\n",
    "\n",
    "        # Logging\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\"| Batch {batch_idx+1:03d}/{len(train_loader):03d}\"\n",
    "              f\"| Train Loss: {loss:.2f}\")\n",
    "\n",
    "    # model.eval()"
   ],
   "id": "fa4d7dcbcbf07e38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003| Batch 001/003| Train Loss: 0.75\n",
      "Epoch: 001/003| Batch 002/003| Train Loss: 0.65\n",
      "Epoch: 001/003| Batch 003/003| Train Loss: 0.42\n",
      "Epoch: 002/003| Batch 001/003| Train Loss: 0.05\n",
      "Epoch: 002/003| Batch 002/003| Train Loss: 0.13\n",
      "Epoch: 002/003| Batch 003/003| Train Loss: 0.00\n",
      "Epoch: 003/003| Batch 001/003| Train Loss: 0.01\n",
      "Epoch: 003/003| Batch 002/003| Train Loss: 0.00\n",
      "Epoch: 003/003| Batch 003/003| Train Loss: 0.02\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the loss reaches 0 after three epochs, a sign that the model converged on the training set. Here, we initialise a model with two inputs and two outputs because our toy dataset has two input features and two class labels to predict. We used a `stochastic gradient descent (SGD)` optimiser with a `learning rate (lr)` of 0.5. The learning rate is a `hyperparameter`, meaning it’s a tunable setting that we must experiment with based on observing the loss. Ideally, we want to choose a learning rate such that the loss converges after a certain number of epochs—the number of epochs is another hyperparameter to choose.",
   "id": "f33ae47296845a0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note about PyTorch optimisers:\n",
    "The optimiser is a crucial component that helps your neural network learn effectively.\n",
    "In the code snippet above `SGD` is `Stochastic Gradient Descent`, one of the most fundamental optimisation algorithms in deep learning. Imagine you're trying to find the lowest point in a valley while blindfolded. SGD is like taking steps in the direction where the ground feels like it's sloping downward. The \"stochastic\" part means you're taking these steps based on looking at just a small part of your data at a time (a batch), rather than the whole dataset at once.\n",
    "\n",
    "Understanding `lr=0.5` (`Learning Rate`):\n",
    "The learning rate is like the size of the steps you take while searching for that lowest point. A learning rate of 0.5 means:\n",
    "- If the gradient (slope) suggests moving in a certain direction, you'll move half that distance\n",
    "- A larger learning rate (like 1.0) means bigger steps - faster learning but risk of overshooting\n",
    "- A smaller learning rate (like 0.01) means smaller steps - more precise but slower learning"
   ],
   "id": "2a4479b1d8fcabe4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note about Epochs and Batches:\n",
    "1. Epoch\n",
    "- Definition: An epoch refers to one complete pass through the entire training dataset. During an epoch, the model sees all the training data exactly once.\n",
    "- Purpose: Repeated passes (multiple epochs) allow the model to learn and refine its parameters.\n",
    "Typically, you train a model for many epochs (e.g., 10, 100, or more), depending on the dataset size and learning requirements.\n",
    "2. Batch\n",
    "- Definition: A batch is a subset of the training data. Instead of processing the entire dataset at once, the data is divided into smaller chunks (batches), and the model updates its parameters after processing each batch.\n",
    "Why Use Batches?:\n",
    "    - Efficiency: It’s computationally cheaper and faster to process smaller chunks of data, especially with large datasets.\n",
    "    - Stability: Helps smooth out updates to the model by averaging gradients over the batch.\n",
    "    - Memory Management: Allows training on large datasets that don’t fit into memory."
   ],
   "id": "2bcbcbd72a36844"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In practice, we often use a third dataset, a so-called `validation dataset`, to find the optimal hyperparameter settings. A validation dataset is similar to a test set. However, while we only want to **use a test set precisely once** to avoid biasing the evaluation, we usually use the validation set multiple times to tweak the model settings.",
   "id": "a7ac696915d8c340"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After we have trained the model, we can use it to make predictions:",
   "id": "9809f900e46fa382"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:44:48.030666Z",
     "start_time": "2024-12-12T12:44:48.022793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "print(outputs)"
   ],
   "id": "bbf9e0e93e1db8f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.9320, -4.2563],\n",
      "        [ 2.6045, -3.8389],\n",
      "        [ 2.1484, -3.2514],\n",
      "        [-2.1461,  2.1496],\n",
      "        [-2.5004,  2.5210]])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To obtain the class membership probabilities, we can then use PyTorch’s `softmax` function:",
   "id": "4b0a138d37793148"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:47:14.503262Z",
     "start_time": "2024-12-12T12:47:14.498271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim=1) # Computes softmax for each row independently.\n",
    "print(probas)"
   ],
   "id": "b8faf07e947cefbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.9992,     0.0008],\n",
      "        [    0.9984,     0.0016],\n",
      "        [    0.9955,     0.0045],\n",
      "        [    0.0134,     0.9866],\n",
      "        [    0.0066,     0.9934]])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's consider the first row:   [0.9992,     0.0008] --> Here, the first value (column) means that the training example has a 99.92% probability of belonging to class 0 and a 0.08% probability of belonging to class 1.",
   "id": "a81a6b2af182fd65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can convert these values into class label predictions using PyTorch’s `argmax` function, which returns the index position of the highest value in each row if we set `dim=1` (setting dim=0 would return the highest value in each column instead):",
   "id": "deef1fced528d9ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T13:00:02.093768Z",
     "start_time": "2024-12-12T13:00:02.087190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = torch.argmax(probas, dim=1)\n",
    "print(predictions)"
   ],
   "id": "5f43150db37919f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We could also apply the argmax function to the logits (outputs) directly:",
   "id": "72a9dc084810337e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T13:02:38.262505Z",
     "start_time": "2024-12-12T13:02:38.257118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = torch.argmax(outputs, dim=1)\n",
    "print(predictions)"
   ],
   "id": "b1481e6fb5139f28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
