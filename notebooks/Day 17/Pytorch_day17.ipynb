{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model training with the `DistributedDataParallel` strategy",
   "id": "b00d571c302eb13a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T16:35:45.344365Z",
     "start_time": "2024-12-17T16:35:45.335465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.xpu import device\n",
    "\n",
    "\n",
    "# Creating a multilayer perceptron with two hidden layers\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "  def __init__(self, num_inputs, num_outputs):\n",
    "    super().__init__()\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        # First hidden layer\n",
    "        torch.nn.Linear(num_inputs, 30),\n",
    "        torch.nn.ReLU(),\n",
    "\n",
    "        # Second hidden layer\n",
    "        torch.nn.Linear(30, 20),\n",
    "        torch.nn.ReLU(),\n",
    "\n",
    "        # Output layer\n",
    "        torch.nn.Linear(20, num_outputs)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "\n",
    "# Creating a small toy dataset\n",
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "x_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],])\n",
    "\n",
    "y_test = torch.tensor([0, 1])\n"
   ],
   "id": "56b765009f4a37d1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T16:35:46.716142Z",
     "start_time": "2024-12-17T16:35:46.709664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining a custom Dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.features = X\n",
    "    self.labels = y\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    one_x = self.features[index]\n",
    "    one_y = self.labels[index]\n",
    "    return one_x, one_y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.labels.shape[0]\n",
    "\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(x_test, y_test)"
   ],
   "id": "6693160a1c7a6801",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T16:35:48.075393Z",
     "start_time": "2024-12-17T16:35:48.066599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiating data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "id": "bbc4492aa64384c5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T16:59:43.652808Z",
     "start_time": "2024-12-17T16:59:43.642900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.utils.data import DistributedSampler\n",
    "\n",
    "\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    \"\"\"Initialise distributed training in PyTorch\n",
    "    This function sets the main node’s address and port to allow for communication between the different processes, initialises the process group with the NCCL backend (designed for GPU-to-GPU communication), and sets the rank (process identifier) and world size (total number of processes). Finally, it specifies the GPU device corresponding to the current model training process rank.\"\"\"\n",
    "\n",
    "    # Environment setup (like a meeting point)\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\" # Address of the main node (where the processes meet on the computer)\n",
    "    os.environ[\"MASTER_ADDR\"] = \"12345\" # Any free port on the machine (which door to use)\n",
    "\n",
    "    # Process Group Initialisation\n",
    "    init_process_group(\n",
    "        backend=\"nccl\", # NVIDIA Collective Communication Library (Communication protocol)\n",
    "        rank=rank, # refers to the index of the GPU we want to use (This process's ID, Giving each GPU worker an ID number)\n",
    "        world_size=world_size # is the number of GPUs to use (total number of processes)\n",
    "    )\n",
    "    torch.cuda.set_device(rank) # Sets the current GPU device on which tensors will be allocated and operations will be performed (This assigns work to specific GPUs: If rank=0, use GPU 0 If rank=1, use GPU 1)"
   ],
   "id": "aa7eb9699679122",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-17T16:44:17.679214Z",
     "start_time": "2024-12-17T16:44:17.667755Z"
    }
   },
   "source": [
    "def prepare_dataset():\n",
    "    # Insert dataset preparation code\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_ds,\n",
    "        batch_size=2,\n",
    "        shuffle=False, # Distributed-Sampler takes care of the shuffling now\n",
    "        pin_memory=True, # Enables faster memory transfer when training on GPU (Makes CPU-to-GPU transfer faster)\n",
    "        drop_last=True, # Drops incomplete final batch - Prevents issues with uneven batch sizes\n",
    "        sampler=DistributedSampler(train_ds) # split the dataset to distinct, non-overlapping subsets for each process(GPU). Each GPU will receive a different subsample of the training data. To ensure this, we set `sampler=DistributedSampler(train_ds)` in the training loader.\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Think of it like dealing cards in a poker game:\n",
    "# - DistributedSampler is the dealer\n",
    "# - Each GPU is a player\n",
    "# - Cards (data) are dealt evenly\n",
    "# - No player (GPU) gets the same cards (data)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:08:33.266422Z",
     "start_time": "2024-12-17T17:08:33.260174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy(model, dataloader, device): # `model` is the trained NN, `dataloader` provides batches of test/validation data\n",
    "    model = model.eval() # Puts the model in evaluation mode (Disables dropout layers, Uses the running statistics in batch normalization)\n",
    "    correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions # Creates a boolean tensor where: `True`--> prediction matches the label, `False`--> prediction was wrong\n",
    "        print(f\" compare is: {compare}\")\n",
    "        correct+= torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return (correct / total_examples).item() # .item() converts from tensor to Python scalar"
   ],
   "id": "38f1cc13760b0df6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main(rank, world_size, num_epochs):\n",
    "    ddp_setup(rank, world_size)\n",
    "    train_loader, test_loader = prepare_dataset()\n",
    "\n",
    "    model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "    model.to(rank) # we now transfer the model and data to the target device\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr = 0.5)\n",
    "    model =  DDP(model,device_ids=[rank])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(rank), labels.to(rank) # rank is the GPU ID\n",
    "            logits = model(features)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            print(f\"[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "                  f\" | Batch size {labels.shape[0]:03d}\"\n",
    "                  f\" | Train/Val Loss: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    train_acc = compute_accuracy(model, train_loader, device=rank)\n",
    "    print(f\"[GPU{rank}] Training accuracy\", train_acc)\n",
    "\n",
    "    test_acc = compute_accuracy(model, test_loader, device=rank)\n",
    "    print(f\"[GPU{rank}] Test accuracy\", test_acc)\n",
    "\n",
    "    destroy_process_group()"
   ],
   "id": "99ed601063ac98ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "    torch.manual_seed(123)\n",
    "    num_epochs = 3\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(main, args=(world_size, num_epochs), nprocs=world_size) # Here, the `spawn` function launches one process per GPU setting `nproces=world_size`, where the world size is the number of available GPUs.\n",
    "    # Note that the main function has a `rank` argument that we don’t include in the `mp.spawn()` call. That’s because the rank, which refers to the process ID we use as the GPU ID, is already passed automatically."
   ],
   "id": "961b76c9d7b57a1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `main` function sets up the distributed environment via `ddp_setup` —another function we defined—loads the training and test sets, sets up the model, and carries out the training. Compared to the single-GPU training , we now transfer the model and data to the target device via `.to(rank)`, which we use to refer to the GPU device ID. Also, we wrap the model via DDP, which enables the synchronisation of the gradients between the different GPUs during training. After the training finishes and we evaluate the models, we use `destroy_process_group()` to cleanly exit the distributed training and free up the allocated resources.\n",
    "\n",
    "Earlier I mentioned that each GPU will receive a different subsample of the training data. To ensure this, we set `sampler=DistributedSampler(train_ds)` in the training loader."
   ],
   "id": "269f7b34a994751b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
