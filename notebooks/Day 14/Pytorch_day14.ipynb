{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A.9.1 PyTorch computations on GPU devices\n",
    "Modifying the training loop to run optionally on a GPU is relatively simple and only requires changing three lines of code (see section A.7). Before we make the modifications, it’s crucial to understand the main concept behind GPU computations within PyTorch.\n",
    "In PyTorch, a device is where computations occur and data resides. The CPU and the GPU are examples of devices. A PyTorch tensor resides in a device, and its operations are executed on the same device.\n",
    "\n",
    "Let’s see how this works in action. Assuming that you installed a GPU-compatible version of PyTorch (see section A.1.3), we can double-check that our runtime indeed supports GPU computing via the following code:"
   ],
   "id": "16826c5b7fc0513b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T09:24:43.369125Z",
     "start_time": "2024-12-14T09:24:43.363286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "# ThIs is False because Apple Silicon chips (like M4, M2, M1) use GPUs that aren't supported by CUDA, which is specific to NVIDIA GPUs. PyTorch can utilise the Apple GPU via Metal Performance Shaders (MPS), Apple's GPU framework."
   ],
   "id": "9166e82b9e03597d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T09:24:45.221924Z",
     "start_time": "2024-12-14T09:24:45.218033Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.backends.mps.is_available())",
   "id": "1e55d8004a69ed3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T09:24:47.170716Z",
     "start_time": "2024-12-14T09:24:47.166386Z"
    }
   },
   "source": [
    "# Set the Device to MPS When running our PyTorch model, specify the device as mps\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, suppose we have two tensors that we can add; this computation will be carried out on the CPU by default:",
   "id": "d83f981556607c98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T09:24:49.657654Z",
     "start_time": "2024-12-14T09:24:49.653233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_1 = torch.tensor([1., 2., 3.])\n",
    "tensor_2 = torch.tensor([4., 5., 6.])\n",
    "print(tensor_1 + tensor_2)"
   ],
   "id": "68a3dff5cc0763b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can now use the `.to()` method. This method is the same as the one we use to change a tensor’s datatype (see 2.2.2) to transfer these tensors onto a GPU and perform the addition there.\n",
    "Move Tensors to the GPU: Use `.to(\"mps\")` or `.to(device)` to transfer tensors to the GP"
   ],
   "id": "13cc2f3d3ba74bdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T09:24:52.349825Z",
     "start_time": "2024-12-14T09:24:52.336218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_1 = tensor_1.to(\"mps\")\n",
    "tensor_2 = tensor_2.to(\"mps\")\n",
    "print(tensor_1 + tensor_2)"
   ],
   "id": "6f575ea74a5a12f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T09:24:53.433804Z",
     "start_time": "2024-12-14T09:24:53.425239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_1 = tensor_1.to(device)\n",
    "tensor_2 = tensor_2.to(device)\n",
    "print(tensor_1 + tensor_2)"
   ],
   "id": "2e51e45fa68efb25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The resulting tensor now includes the device information, `device='cuda:0'`, which means that the tensors reside on the first GPU. If your machine hosts multiple GPUs, you can specify which GPU you’d like to transfer the tensors to. You do so by indicating the device ID in the transfer command. For instance, you can use `.to(\"cuda:0\")`, `.to(\"cuda:1\")`, and so on.\n",
    "\n",
    "On Apple Silicon using MPS (Metal Performance Shaders), the concept of multiple GPUs does not apply because Apple Silicon devices typically have only one integrated GPU accessible via MPS. This simplifies device selection compared to CUDA-based systems.\n",
    "\n",
    "However, all tensors must be on the same device. Otherwise, the computation will fail, where one tensor resides on the CPU and the other on the GPU:"
   ],
   "id": "b27902b0d32930e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T09:25:03.162676Z",
     "start_time": "2024-12-14T09:25:03.136205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_1 = tensor_1.to(\"cpu\")\n",
    "print(tensor_1 + tensor_2)"
   ],
   "id": "688187ddf5492904",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m tensor_1 \u001B[38;5;241m=\u001B[39m tensor_1\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtensor_1\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtensor_2\u001B[49m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
