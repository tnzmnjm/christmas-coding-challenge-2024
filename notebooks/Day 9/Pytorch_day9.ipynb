{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVJtwbyg52p15irx3h26yb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Softmax function**\n",
        "\n",
        "It converts a vector of numbers into a probability distribution.\n",
        "\n",
        "\n",
        "**Mathematical Definition:**\n",
        "\n",
        "For a `vector z` with `K elements`, the softmax function is defined as:\n",
        "\n",
        "\n",
        "   $$ \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} $$\n",
        "\n",
        "**Key Properties:**\n",
        "\n",
        "\n",
        "- Outputs are always between 0 and 1\n",
        "- The sum of all outputs equals 1\n",
        "- Preserves the relative ordering of the input elements\n",
        "- Differentiable, making it suitable for gradient-based optimisation\n",
        "\n",
        "\n",
        "**Common Applications:**\n",
        "- Neural Network Output Layer:\n",
        "  - Used in the final layer of classification networks\n",
        "  - Converts raw scores into class probabilities\n",
        "\n",
        "\n",
        "- Attention Mechanisms:\n",
        "\n",
        "  - Used in transformer models to compute attention weights\n",
        "  - Helps in determining the importance of different input elements\n",
        "\n",
        "\n",
        "- Reinforcement Learning:\n",
        "\n",
        "  - Converting action scores into action probabilities\n",
        "  - Used in policy\n",
        "  \n",
        "**Key Implementation Note:**\n",
        "When implementing softmax, it's common to subtract the maximum value from all inputs first:\n",
        "\n",
        "  $$ \\text{softmax}(z_i) = \\frac{e^{z_i-max(z)}}{\\sum_{j=1}^{K} e^{z_j-max(z)}} $$"
      ],
      "metadata": {
        "id": "mrcfK2xNPWOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyTorch's commonly used loss functions**\n",
        "\n",
        "- **Classification Loss Functions:**\n",
        "Used when the target is a class label or probability distribution over classes.\n",
        "  1.   Cross Entropy Loss (nn.CrossEntropyLoss)\n",
        "  2.   Binary Cross Entropy Loss (nn.BCELoss)\n",
        "  3.   NLLLoss (Negative Log-Likelihood Loss)\n",
        "  4.   KLDivLoss (Kullback-Leibler Divergence Loss)\n",
        "\n",
        "- **Regression Loss Functions**\n",
        "  5.   Mean Squared Error Loss (nn.MSELoss)\n",
        "  6.   L1 Loss (nn.L1Loss)\n",
        "  7.   SmoothL1Loss (Huber Loss)\n",
        "  \n"
      ],
      "metadata": {
        "id": "78lMKkZbWu4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Entropy Loss (nn.CrossEntropyLoss)\n",
        "\n",
        "  $$CrossEentropyLoss = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n",
        "\n",
        "\n",
        "- whaere $y_i$ is the true label and $\\hat{y}_i$ is the predicted probbility.\n",
        "- Use case: `Multi-class classification`.\n",
        "- Combines `LogSoftmax` and `Negative Log-Likelihood Loss (NLLLoss)` into one.\n",
        "- Automatically applies softmax to your network output\n",
        "- Input: Logits (raw, unnormalised scores) of shape (N, C) where\n",
        "N is batch size, and C is the number of classes.\n",
        "- Target: Class indices (integers from 0 to Câˆ’1).\n",
        "- Target shape: (batch_size, ...)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EqDSF4yNZ1Y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Cross Entropy Loss (nn.BCELoss)\n",
        "\n",
        "$$BCELoss = - (y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i))$$\n",
        "\n",
        "- Used for `binary classification`\n",
        "- Expects input to be sigmoid output (between 0 and 1)\n",
        "- Often paired with sigmoid activation\n",
        "- Input/target shape: (batch_size, ...)"
      ],
      "metadata": {
        "id": "4IgGstefdwb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLLLoss (Negative Log-Likelihood Loss)\n",
        "\n",
        "\n",
        "$$NLLLoss = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i)$$\n",
        "\n",
        "- N is batch size\n",
        "- Used with log-softmax output\n",
        "- Use case: `Multi-class classification`, but requires log probabilities as input.\n",
        "- Usually used through CrossEntropyLoss, used less often as `CrossEntropyLoss` handles the same task more efficiently.\n",
        "- Expects input to be log probabilities\n",
        "- Input shape: (batch_size, num_classes, ...)\n",
        "- Target shape: (batch_size, ...)"
      ],
      "metadata": {
        "id": "_Uf0Z3h9fMpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KLDivLoss (Kullback-Leibler Divergence Loss)\n",
        "\n",
        "\n",
        "$$KLDivLoss = \\sum_{i=1}^{N} y_i \\log(\\frac{y_i}{\\hat{y}_i})$$\n",
        "\n",
        "- Measures difference between two probability distributions\n",
        "- Used in distillation and probabilistic learning\n",
        "- Input: Log-probabilities (output from log_softmax).\n",
        "- target: Probabilities (must sum to 1)."
      ],
      "metadata": {
        "id": "7YFmIf2tmOrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Squared Error Loss\n",
        "\n",
        "\n",
        "$$MSELoss = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "- Used for regression problems\n",
        "- Measures average squared difference between predictions and targets\n",
        "- More sensitive to outliers than L1Loss\n",
        "- Input/target shape: any matching shapes"
      ],
      "metadata": {
        "id": "RTWKonyvkccp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L1Loss (Mean Absolute Error Loss)\n",
        "\n",
        "\n",
        "$$L1Loss = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$\n",
        "\n",
        "- Used for regression\n",
        "- More robust to outliers than MSELoss\n",
        "- Input/target shape: any matching shapes\n",
        "- Less sensitive to outliers compared to MSELoss."
      ],
      "metadata": {
        "id": "w-fYI05JlEpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SmoothL1Loss (Huber Loss)\n",
        "\n",
        "\n",
        "$$SmoothL1Loss = \\begin{cases}\n",
        "    0.5 (y_i - \\hat{y}_i)^2, & \\text{if } |y_i - \\hat{y}_i| < 1 \\\\\n",
        "    |y_i - \\hat{y}_i| - 0.5, & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "- Use case: Regression tasks, robust to outliers by combining L1 and L2 loss.\n",
        "- L1 loss for large errors, L2 loss for small errors.\n",
        "- Used in: Object detection (e.g., Faster R-CNN)\n",
        "and Regression tasks requiring robustness\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M6amBxcSnbJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tips for Choosing Loss Functions:\n",
        "\n",
        "- Classification Tasks:\n",
        "\n",
        "    - Binary: Use BCEWithLogitsLoss\n",
        "    - Multi-class: Use CrossEntropyLoss\n",
        "    - Multi-label: Use BCEWithLogitsLoss\n",
        "\n",
        "\n",
        "- Regression Tasks:\n",
        "\n",
        "    - General purpose: MSELoss\n",
        "    - Outlier-robust: L1Loss or SmoothL1Loss\n",
        "    - Distribution learning: KLDivLoss"
      ],
      "metadata": {
        "id": "Ox0Z7LSEou_j"
      }
    }
  ]
}