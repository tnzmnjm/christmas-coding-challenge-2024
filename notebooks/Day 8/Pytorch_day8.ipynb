{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPITMh+k3Eb+iTw0h7NMEs2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JMU9eEswM3Fs"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a multilayer perceptron with two hidden layers\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "  def __init__(self, num_inputs, num_outputs):\n",
        "    super().__init__()\n",
        "    self.layers = torch.nn.Sequential(\n",
        "        # First hidden layer\n",
        "        torch.nn.Linear(num_inputs, 30),\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        # Second hidden layer\n",
        "        torch.nn.Linear(30, 20),\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        # Output layer\n",
        "        torch.nn.Linear(20, num_outputs)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    logits = self.layers(x)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "1oVnHNTTNTdP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a new neural network object\n",
        "model = NeuralNetwork(50, 3)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fn-RG8YQyCI",
        "outputId": "8cc988aa-43fc-4fbd-a78a-f6764764025f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layers[0].weight)\n",
        "print(model.layers[0].weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqpqB7_WRdSx",
        "outputId": "2a5450a5-9e4f-464e-a092-8a9771416e62"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0851, -0.0246,  0.0369,  ..., -0.0976, -0.0555,  0.0996],\n",
            "        [-0.1009, -0.0855, -0.1353,  ...,  0.1170, -0.0491, -0.1183],\n",
            "        [-0.0175, -0.0101, -0.1335,  ..., -0.0947, -0.0027,  0.1162],\n",
            "        ...,\n",
            "        [-0.1405,  0.0285, -0.0759,  ...,  0.0632, -0.0161,  0.0787],\n",
            "        [ 0.0219, -0.1214, -0.0889,  ..., -0.0393,  0.0574,  0.0632],\n",
            "        [-0.1153,  0.0546, -0.0619,  ..., -0.0803, -0.0399,  0.0012]],\n",
            "       requires_grad=True)\n",
            "torch.Size([30, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model weights are initialised with small random numbers, which differ each time we instantiate the network. In deep learning, initialising model weights with small random numbers is desired to break symmetry during training. Otherwise, the nodes would be performing the same operations and updates during backpropagation, which would not allow the network to learn complex mappings from inputs to outputs.\n",
        "\n",
        "However, while we want to keep using small random numbers as initial values for our layer weights, we can make the random number initialisation reproducible by seeding PyTorch’s random number generator via `manual_seed`:\n"
      ],
      "metadata": {
        "id": "nMNctlNDR0YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = NeuralNetwork(50, 3)\n",
        "print(model.layers[0].weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcHcRXnpSh0u",
        "outputId": "4954797d-6a37-46d0-f9bc-9b88b56e24da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
            "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
            "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
            "        ...,\n",
            "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
            "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
            "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have spent some time inspecting the `NeuralNetwork` instance, let’s briefly see how it’s used via the forward pass:"
      ],
      "metadata": {
        "id": "zl3fnb3AS1zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "x = torch.rand(1, 50)\n",
        "# creates a PyTorch tensor x with random values drawn from a uniform\n",
        "# distribution on the interval [0, 1). The tensor has 1 row and 50 columns,\n",
        "# making it a 1x50 tensor\n",
        "\n",
        "out = model(x) # Will automatically execute  the forward pass of the model.\n",
        "\n",
        "print(out) # These three numbers returned here correspond to a score assigned\n",
        "# to each of the three output nodes."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwrAePOuTCYa",
        "outputId": "f803a0b7-4d9a-4388-da2d-c9b5d3964977"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the output tensor also includes a `grad_fn` value.\n",
        "\n",
        "Here, `grad_fn=<AddmmBackward0>` represents the last-used function to compute a variable in the computational graph. In particular, `grad_fn=<AddmmBackward0>` means that the tensor we are inspecting was created via a `matrix multiplication` and `addition` operation. PyTorch will use this information when it computes gradients during backpropagation.\n",
        "\n",
        "The `<AddmmBackward0>` part of `grad_fn=<AddmmBackward0>` specifies the operation performed. In this case, it is an `Addmm` operation. `Addmm` stands for `matrix multiplication (mm)` followed by an `addition (Add)`."
      ],
      "metadata": {
        "id": "qTe17BCRVgKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we just want to use a network without training or backpropagation—for example, if we use it for prediction after training—constructing this computational graph for backpropagation can be wasteful as it performs unnecessary computations and consumes additional memory. So, when we use a model for inference (for instance, making predictions) rather than training, the best practice is to use the `torch.no_grad()` context manager. This tells PyTorch that it doesn’t need to keep track of the gradients, which can result in significant savings in memory and computation:"
      ],
      "metadata": {
        "id": "2vPTranjWM-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out = model(x)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chF_r-jVWl1Z",
        "outputId": "c783bd6e-bfcd-415a-aa26-b79c4c1c8f22"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1262,  0.1080, -0.1792]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Logits` are the **raw**, **unnormalised** scores produced by the last layer of a neural network before applying any non-linear activation like softmax or sigmoid. These values can be positive or negative and are not constrained to a specific range.\n",
        "\n",
        "In PyTorch, models often output **logits (raw scores** from the final layer) **rather than probabilities** for classification tasks.\n",
        "\n",
        "In PyTorch, it’s common practice to code models such that they return the outputs of the last layer (`logits`) without passing them to a nonlinear activation function. That’s because PyTorch’s commonly used loss functions combine the `softmax` (or `sigmoid` for binary classification) operation with the negative log-likelihood loss in a single class. The reason for this is numerical efficiency and stability. So, if we want to compute class-membership probabilities for our predictions, we have to call the `softmax` function explicitly."
      ],
      "metadata": {
        "id": "W3BlnMKoXhXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out = torch.softmax(model(x), dim=1)\n",
        "  print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKgA-e2AfwPq",
        "outputId": "c18fc5c1-d21f-4b2f-b471-ee4aca75f559"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3113, 0.3934, 0.2952]])\n"
          ]
        }
      ]
    }
  ]
}