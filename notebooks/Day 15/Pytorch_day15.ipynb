{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A.9.2 Single-GPU training\n",
    "\n",
    "Now that we are familiar with transferring tensors to the GPU, we can modify the training loop to run on a GPU. This step requires only changing three lines of code, as shown in the following listing.\n"
   ],
   "id": "5fadb77a09afe99c"
  },
  {
   "cell_type": "code",
   "id": "8d02238db48fcce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:44:30.272726Z",
     "start_time": "2024-12-15T13:44:30.265295Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Creating a multilayer perceptron with two hidden layers\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "  def __init__(self, num_inputs, num_outputs):\n",
    "    super().__init__()\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        # First hidden layer\n",
    "        torch.nn.Linear(num_inputs, 30),\n",
    "        torch.nn.ReLU(),\n",
    "\n",
    "        # Second hidden layer\n",
    "        torch.nn.Linear(30, 20),\n",
    "        torch.nn.ReLU(),\n",
    "\n",
    "        # Output layer\n",
    "        torch.nn.Linear(20, num_outputs)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "\n",
    "# Creating a small toy dataset\n",
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "x_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],])\n",
    "\n",
    "y_test = torch.tensor([0, 1])\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:44:50.615902Z",
     "start_time": "2024-12-15T13:44:50.611408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining a custom Dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.features = X\n",
    "    self.labels = y\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    one_x = self.features[index]\n",
    "    one_y = self.labels[index]\n",
    "    return one_x, one_y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.labels.shape[0]\n",
    "\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(x_test, y_test)"
   ],
   "id": "6cf7f0cb9e29de79",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T13:45:13.104529Z",
     "start_time": "2024-12-15T13:45:13.098445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Instantiating data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:48:15.276547Z",
     "start_time": "2024-12-15T13:48:12.876993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#  A training loop on a GPU\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "\n",
    "device = torch.device('mps')\n",
    "model = model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(),\n",
    "                            lr=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx+1:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train Loss: {loss:.2f}\")"
   ],
   "id": "ebb30f56ae2fc70c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 001/003 | Train Loss: 0.75\n",
      "Epoch: 001/003 | Batch 002/003 | Train Loss: 0.65\n",
      "Epoch: 001/003 | Batch 003/003 | Train Loss: 0.42\n",
      "Epoch: 002/003 | Batch 001/003 | Train Loss: 0.05\n",
      "Epoch: 002/003 | Batch 002/003 | Train Loss: 0.13\n",
      "Epoch: 002/003 | Batch 003/003 | Train Loss: 0.00\n",
      "Epoch: 003/003 | Batch 001/003 | Train Loss: 0.01\n",
      "Epoch: 003/003 | Batch 002/003 | Train Loss: 0.00\n",
      "Epoch: 003/003 | Batch 003/003 | Train Loss: 0.02\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Running the preceding code will output the following, similar to the results obtained on the CPU.\n",
    "\n",
    "We can use `.to(\"cuda\")` | `.to(\"mps\")` instead of `device = torch.device(\"cuda\")` | `device = torch.device('mps')`.\n",
    "Transferring a tensor to `\"cuda\"` instead of `torch.device(\"cuda\")` works as well and is shorter. We can also modify the statement, which will make the same code executable on a CPU if a GPU is not available. This is considered best practice when sharing PyTorch code:\n",
    "\n"
   ],
   "id": "3435e876ea43cc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ],
   "id": "66d1d6021f454183"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the case of the modified training loop here, we probably wonâ€™t see a speedup due to the memory transfer cost from CPU to GPU. However, we can expect a significant speedup when training deep neural networks, especially LLMs.",
   "id": "806481be39224dd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7dad1dd43e3c13b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
