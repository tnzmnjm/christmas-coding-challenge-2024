{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJiKHLaWjpT7ZnL/80Yqz7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnzmnjm/christmas-coding-challenge-2024/blob/main/notebooks/Day%203/Pytorch_day3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MxtjRVJ8QkEI"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A.2.3 Common PyTorch tensor operations**\n",
        "\n",
        "Comprehensive coverage of all the different PyTorch tensor operations and commands is outside the scope of this book. However, I will briefly describe relevant operations as we introduce them throughout the book.\n",
        "\n",
        "We have already introduced the `torch.tensor()` function to create new tensors:"
      ],
      "metadata": {
        "id": "viTuJ0j8Q5WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(tensor2d)\n",
        "print(tensor2d.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s71s-VoRAoT",
        "outputId": "01ea4358-c45c-4560-f998-668409e33423"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reshaping tensors is an essential operation in machine learning and numerical computations because it allows you to adapt the data to the requirements of specific algorithms or models without changing the underlying data. In my case, as I'm learning PyTorch while following Build LLM from Scratch book, reshaping tensors will likely come up often because different steps in building and training a model (especially for deep learning) require tensors in specific shapes.\n",
        "\n",
        "\n",
        "As you can see, `.shape` returns `[2, 3]`, meaning the tensor has two rows and three columns. To reshape the tensor into a `3 × 2` tensor, we can use the `.reshape` method:"
      ],
      "metadata": {
        "id": "BnK-xbtJRk-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2d.reshape(3, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-mm87scSStB",
        "outputId": "6acea7d3-2a9c-4fa9-8890-90f643960d17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, note that the more common command for reshaping tensors in PyTorch is `.view()`:"
      ],
      "metadata": {
        "id": "QFtrijLGS8bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2d.view(3, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubUmykmDS7xf",
        "outputId": "e98d3b84-c12c-428b-e1a5-5168e2af127f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to `.reshape` and `.view`, in several cases, `PyTorch` offers multiple syntax options for executing the same computation. PyTorch initially followed the original `Lua Torch`¹ syntax convention but then, by popular request, added syntax to make it similar to `NumPy`. (The subtle difference between `.view()` and `.reshape()` in PyTorch lies in their handling of memory layout: `.view()` requires the original data to be contiguous and will fail if it isn’t, whereas `.reshape()` will work regardless, copying the data if necessary to ensure the desired shape.)"
      ],
      "metadata": {
        "id": "JxILycPF9mhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transpose a Tensor**\n",
        "\n",
        "Transposing reorders the dimensions (axes) of a tensor. For example:\n",
        "- A 2D tensor (matrix) with shape (rows, columns) becomes (columns, rows).\n",
        "- For higher-dimensional tensors, specific axes are swapped.\n",
        "\n",
        "We may need to transpose tensors for various reasons:\n",
        "\n",
        "1.   to align the dimensions of two tensors for matrix multiplication.\n",
        "2.   In an LLM, we might have:\n",
        "  - Input sequence embeddings with shape `(batch_size, sequence_length, embedding_dim)`.\n",
        "\n",
        "  - A weight matrix with shape `(embedding_dim, hidden_dim)`.\n",
        "3. To multiply, we may need to transpose the weight matrix to`(hidden_dim, embedding_dim)`\n",
        "\n",
        "\n",
        "We can use `.T ` to transpose a tensor, which means flipping it across its diagonal. Note that this is similar to reshaping a tensor, as you can see based on the following result:"
      ],
      "metadata": {
        "id": "8pr5tI5-ACF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2d.shape)\n",
        "print(tensor2d.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "810zv0e9BtAJ",
        "outputId": "9e68076c-5896-4dd0-d974-5859a2ef82ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrices Multiplication**\n",
        "\n",
        "I reccommend watching these two videos from 3Blue1Brown youtube channel:\n",
        "  - [Linear transformations and matrices ](https://www.youtube.com/watch?v=kYB8IZa5AuE&ab_channel=3Blue1Brown)\n",
        "  - [Matrix multiplication as composition](https://www.youtube.com/watch?v=XkY2DOUCWMU&ab_channel=3Blue1Brown)\n",
        "\n",
        "  Some notes from the videos:\n",
        "    - Matrix multiplication means applying a transformation and then another transformation on a matrix.\n",
        "    - in Matrix Multiplication order is important (A B != B A)\n",
        "    - Matric Multiplication is associative ((AB)C = A(BC))\n",
        "    \n",
        "The common way to multiply two matrices in PyTorch is the `.matmul` method"
      ],
      "metadata": {
        "id": "dNOTQEa8EZH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2d.matmul(tensor2d.T))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYC1wKN7M6o6",
        "outputId": "78a3cb1c-9088-4283-8a7a-3d5359be0b21"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[14, 32],\n",
            "        [32, 77]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `Lua Torch` syntax : Lua Torch was a powerful framework with a compact and expressive syntax, especially for defining neural networks and performing tensor operations. Its key features like GPU support, flexibility, and modular design influenced the development of PyTorch. While Lua Torch is largely obsolete, its core ideas live on in modern frameworks like PyTorch."
      ],
      "metadata": {
        "id": "BdV484O_-zVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2d)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4fsUTptPTuq",
        "outputId": "4c69e3e5-41a5-44e7-9935-f36e47ef19f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can also adopt the `@` operator, which accomplishes the same thing more compactly:"
      ],
      "metadata": {
        "id": "Hpg39IbEPk5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2d @ tensor2d.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTuZcSzePoHZ",
        "outputId": "6b1319e7-55a1-4405-a998-bc1210162305"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[14, 32],\n",
            "        [32, 77]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to browse through all the different tensor operations available in `PyTorch` you can check out the official documentation at https://pytorch.org/docs/stable/tensors.html."
      ],
      "metadata": {
        "id": "6k_qmghLP1kE"
      }
    }
  ]
}