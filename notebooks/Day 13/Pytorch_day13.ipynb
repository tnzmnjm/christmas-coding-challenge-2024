{
 "cells": [
  {
   "cell_type": "code",
   "id": "6b7308edab8cc296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:38:16.066118Z",
     "start_time": "2024-12-13T10:38:16.063762Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Creating a multilayer perceptron with two hidden layers\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "  def __init__(self, num_inputs, num_outputs):\n",
    "    super().__init__()\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        # First hidden layer\n",
    "        torch.nn.Linear(num_inputs, 30),\n",
    "        torch.nn.ReLU(),\n",
    "\n",
    "        # Second hidden layer\n",
    "        torch.nn.Linear(30, 20),\n",
    "        torch.nn.ReLU(),\n",
    "\n",
    "        # Output layer\n",
    "        torch.nn.Linear(20, num_outputs)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-12-13T10:39:38.125580Z",
     "start_time": "2024-12-13T10:39:38.121046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating a small toy dataset\n",
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "x_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],])\n",
    "\n",
    "y_test = torch.tensor([0, 1])"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9694db86ec229f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:39:47.473587Z",
     "start_time": "2024-12-13T10:39:47.470264Z"
    }
   },
   "source": [
    "# Defining a custom Dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.features = X\n",
    "    self.labels = y\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    one_x = self.features[index]\n",
    "    one_y = self.labels[index]\n",
    "    return one_x, one_y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.labels.shape[0]\n",
    "\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(x_test, y_test)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "bdda07a1bb232fbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:40:38.409134Z",
     "start_time": "2024-12-13T10:40:38.394748Z"
    }
   },
   "source": [
    "# Instantiating data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:48:42.520910Z",
     "start_time": "2024-12-13T10:48:42.504705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Neural Network training in PyTorch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "optimiser = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.5\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        logits = model(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()   # The optimiser uses the gradients to update the model parameters making them slightly better for next time to minimise the loss. In the case of the SGD optimiser, this means multiplying the gradients with the learning rate and adding the scaled negative gradient to the parameters.\n",
    "\n",
    "        # Logging\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx+1:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train Loss: {loss:.2f}\")\n",
    "\n",
    "    # model.eval()"
   ],
   "id": "fa4d7dcbcbf07e38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 001/003 | Train Loss: 0.75\n",
      "Epoch: 001/003 | Batch 002/003 | Train Loss: 0.65\n",
      "Epoch: 001/003 | Batch 003/003 | Train Loss: 0.42\n",
      "Epoch: 002/003 | Batch 001/003 | Train Loss: 0.05\n",
      "Epoch: 002/003 | Batch 002/003 | Train Loss: 0.13\n",
      "Epoch: 002/003 | Batch 003/003 | Train Loss: 0.00\n",
      "Epoch: 003/003 | Batch 001/003 | Train Loss: 0.01\n",
      "Epoch: 003/003 | Batch 002/003 | Train Loss: 0.00\n",
      "Epoch: 003/003 | Batch 003/003 | Train Loss: 0.02\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After we have trained the model, we can use it to make predictions:",
   "id": "9809f900e46fa382"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:50:03.794978Z",
     "start_time": "2024-12-13T10:50:03.790196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "print(outputs)"
   ],
   "id": "bbf9e0e93e1db8f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.9320, -4.2563],\n",
      "        [ 2.6045, -3.8389],\n",
      "        [ 2.1484, -3.2514],\n",
      "        [-2.1461,  2.1496],\n",
      "        [-2.5004,  2.5210]])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To obtain the class membership probabilities, we can then use PyTorch’s `softmax` function:",
   "id": "4b0a138d37793148"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:50:13.230086Z",
     "start_time": "2024-12-13T10:50:13.225476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim=1) # Computes softmax for each row independently.\n",
    "print(probas)"
   ],
   "id": "b8faf07e947cefbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.9992,     0.0008],\n",
      "        [    0.9984,     0.0016],\n",
      "        [    0.9955,     0.0045],\n",
      "        [    0.0134,     0.9866],\n",
      "        [    0.0066,     0.9934]])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can convert these values into class label predictions using PyTorch’s `argmax` function, which returns the index position of the highest value in each row if we set `dim=1` (setting dim=0 would return the highest value in each column instead):",
   "id": "deef1fced528d9ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:52:47.313624Z",
     "start_time": "2024-12-13T10:52:47.308434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = torch.argmax(probas, dim=1)\n",
    "print(predictions)"
   ],
   "id": "5f43150db37919f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We could also apply the argmax function to the logits (outputs) directly:",
   "id": "72a9dc084810337e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:52:49.506890Z",
     "start_time": "2024-12-13T10:52:49.502279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = torch.argmax(outputs, dim=1)\n",
    "print(predictions)"
   ],
   "id": "b1481e6fb5139f28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, we computed the predicted labels for the training dataset. Since the training dataset is relatively small, we could compare it to the true training labels by eye and see that the model is 100% correct. We can double-check this using the `==` comparison operator:",
   "id": "12867a17770bea04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:53:00.095410Z",
     "start_time": "2024-12-13T10:53:00.085041Z"
    }
   },
   "cell_type": "code",
   "source": "predictions == y_train",
   "id": "a4e420bbc2a1e3bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using `torch.sum`, we can count the number of correct predictions:",
   "id": "f340f1acfb2ffc2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:53:55.428384Z",
     "start_time": "2024-12-13T10:53:55.420671Z"
    }
   },
   "cell_type": "code",
   "source": "torch.sum(predictions == y_train)",
   "id": "73a4ea367e51f053",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:55:51.032305Z",
     "start_time": "2024-12-13T10:55:51.026623Z"
    }
   },
   "cell_type": "code",
   "source": "type(predictions)",
   "id": "3b4eb70164bc5a55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:56:01.675910Z",
     "start_time": "2024-12-13T10:56:01.672461Z"
    }
   },
   "cell_type": "code",
   "source": "type(y_train)",
   "id": "25a5fa4cd306c6bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since the dataset consists of five training examples, we have five out of five predictions that are correct, which has 5/5 × 100% = 100% prediction accuracy. To generalise the computation of the prediction accuracy, let’s implement a compute_accuracy function, as shown in the following listing.",
   "id": "96889002a00eb426"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:18:49.652117Z",
     "start_time": "2024-12-13T11:18:49.648645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy(model, dataloader): # `model` is the trained NN, `dataloader` provides batches of test/validation data\n",
    "    model = model.eval() # Puts the model in evaluation mode (Disables dropout layers, Uses the running statistics in batch normalization)\n",
    "    correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions # Creates a boolean tensor where: `True`--> prediction matches the label, `False`--> prediction was wrong\n",
    "        print(f\" compare is: {compare}\")\n",
    "        correct+= torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return (correct / total_examples).item() # .item() converts from tensor to Python scalar"
   ],
   "id": "fdf4d6fe97f49208",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code iterates over a data loader to compute the number and fraction of the correct predictions. When we work with large datasets, we typically can only call the model on a small part of the dataset due to memory limitations. The `compute_accuracy` function here is a general method that scales to datasets of arbitrary size since, in each iteration, the dataset chunk that the model receives is the same size as the batch size seen during training. The internals of the `compute_accuracy` function are similar to what we used before when we converted the logits to the class labels.",
   "id": "87c1d3ded092458e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can then apply the function to the training:",
   "id": "edc7a2eee02d77cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:18:51.586018Z",
     "start_time": "2024-12-13T11:18:51.581833Z"
    }
   },
   "cell_type": "code",
   "source": "print(compute_accuracy(model, train_loader))",
   "id": "b1662a4b0d179784",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " compare is: tensor([True, True])\n",
      " compare is: tensor([True, True])\n",
      " compare is: tensor([True])\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Similarly, we can apply the function to the test set:",
   "id": "f7a5ce60c0fa07e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:19:46.226701Z",
     "start_time": "2024-12-13T11:19:46.221012Z"
    }
   },
   "cell_type": "code",
   "source": "print(compute_accuracy(model, test_loader))",
   "id": "de6cbc3352cfb869",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " compare is: tensor([True, True])\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### A.8 Saving and loading models",
   "id": "807630d7c9e4c50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:25:01.174118Z",
     "start_time": "2024-12-13T11:25:01.169663Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "58f7c2af1f252bbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:24:31.784573Z",
     "start_time": "2024-12-13T11:24:31.770678Z"
    }
   },
   "cell_type": "code",
   "source": "model.state_dict()",
   "id": "1a105bf4fd25d32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layers.0.weight',\n",
       "              tensor([[-0.3094,  0.1056],\n",
       "                      [-0.3556,  0.2842],\n",
       "                      [-0.6042,  0.5255],\n",
       "                      [-0.5140, -0.5622],\n",
       "                      [-0.4625,  0.3818],\n",
       "                      [-0.2798,  0.3371],\n",
       "                      [-0.6001, -0.4290],\n",
       "                      [-0.2596, -0.1390],\n",
       "                      [-0.5326,  0.4366],\n",
       "                      [-0.1789,  0.2748],\n",
       "                      [ 0.5309,  0.0905],\n",
       "                      [ 0.1360,  0.7413],\n",
       "                      [-0.3296,  0.2661],\n",
       "                      [-0.3307,  0.5679],\n",
       "                      [ 0.6438, -0.6922],\n",
       "                      [ 0.5157,  0.3847],\n",
       "                      [ 0.2495,  0.3144],\n",
       "                      [ 0.6622, -0.1331],\n",
       "                      [-0.5989, -0.2006],\n",
       "                      [-0.5107,  0.0958],\n",
       "                      [-0.0131, -0.4395],\n",
       "                      [-0.0753,  0.7126],\n",
       "                      [ 0.0250, -0.0593],\n",
       "                      [-0.1309,  0.1919],\n",
       "                      [ 0.6089,  0.4868],\n",
       "                      [ 0.2638,  0.1394],\n",
       "                      [ 0.1405,  0.3764],\n",
       "                      [-0.0656, -0.6713],\n",
       "                      [-0.4396,  0.6123],\n",
       "                      [ 0.1011, -0.6060]])),\n",
       "             ('layers.0.bias',\n",
       "              tensor([ 0.1414,  0.5907, -0.6656, -0.4761, -0.2592,  0.0547, -0.1649, -0.0777,\n",
       "                      -0.6989,  0.3485,  0.6186,  0.4243, -0.4854, -0.5473,  0.3259, -0.0194,\n",
       "                       0.2617, -0.6621, -0.0414,  0.5165,  0.3769,  0.0413,  0.1506, -0.0376,\n",
       "                      -0.3735, -0.2658, -0.3931,  0.5971,  0.3923,  0.2686])),\n",
       "             ('layers.2.weight',\n",
       "              tensor([[    -0.1765,     -0.0104,     -0.0568,      0.1263,     -0.1600,\n",
       "                           -0.0521,      0.1783,      0.1228,      0.0534,     -0.1420,\n",
       "                            0.3644,     -0.3461,     -0.0169,     -0.2034,      0.4240,\n",
       "                            0.0576,      0.0215,      0.3330,      0.0904,     -0.0952,\n",
       "                            0.2477,     -0.2786,      0.1001,     -0.0475,      0.0332,\n",
       "                            0.0573,     -0.1596,      0.2354,     -0.1664,      0.3470],\n",
       "                      [    -0.0577,      0.0623,      0.1797,     -0.0783,      0.1714,\n",
       "                           -0.0928,      0.0805,      0.0715,      0.0487,      0.1420,\n",
       "                           -0.0749,      0.0434,      0.0001,     -0.1397,     -0.0443,\n",
       "                           -0.1228,      0.0790,      0.0164,      0.0179,     -0.0576,\n",
       "                            0.0009,     -0.0609,      0.0519,      0.1766,      0.0270,\n",
       "                           -0.0809,     -0.1120,      0.0140,     -0.1406,     -0.1372],\n",
       "                      [    -0.1172,     -0.0639,      0.0126,      0.1257,      0.0716,\n",
       "                            0.1402,     -0.1146,      0.0154,     -0.1647,      0.1050,\n",
       "                           -0.0529,      0.1699,     -0.1295,      0.1474,      0.0279,\n",
       "                            0.0920,     -0.0115,      0.1243,      0.1220,      0.1363,\n",
       "                           -0.0360,     -0.1409,     -0.0074,      0.0366,     -0.1507,\n",
       "                           -0.1131,     -0.1721,     -0.1241,      0.1562,     -0.0220],\n",
       "                      [    -0.0112,      0.0073,      0.1138,     -0.1612,     -0.1409,\n",
       "                           -0.0607,     -0.1051,      0.0942,      0.1290,     -0.1771,\n",
       "                           -0.1549,     -0.1778,      0.0689,      0.1469,     -0.1416,\n",
       "                           -0.0845,      0.0581,     -0.1192,      0.1551,      0.0426,\n",
       "                           -0.0508,      0.0119,      0.0569,     -0.0646,     -0.1415,\n",
       "                            0.0012,      0.0033,      0.0037,     -0.0266,      0.1172],\n",
       "                      [    -0.0509,     -0.0177,      0.0751,     -0.1149,      0.0489,\n",
       "                           -0.0404,      0.0876,     -0.0990,      0.0067,      0.0179,\n",
       "                           -0.1469,     -0.1328,      0.0700,     -0.0531,      0.1084,\n",
       "                           -0.1804,     -0.0903,     -0.1504,      0.0729,     -0.0053,\n",
       "                           -0.0341,     -0.0304,     -0.1427,      0.0518,      0.0046,\n",
       "                           -0.1260,      0.0687,     -0.0037,     -0.1766,      0.0982],\n",
       "                      [     0.1420,      0.1226,     -0.0068,      0.0073,      0.2352,\n",
       "                            0.2660,     -0.1378,     -0.0825,      0.2342,      0.2018,\n",
       "                            0.0012,      0.1225,      0.1935,     -0.0389,     -0.2537,\n",
       "                           -0.0478,      0.0495,     -0.2187,      0.1677,      0.2811,\n",
       "                           -0.1729,      0.2037,     -0.0857,      0.2154,      0.0907,\n",
       "                            0.0916,      0.0894,     -0.1348,      0.3391,     -0.1002],\n",
       "                      [    -0.0666,      0.0943,      0.0573,     -0.0473,     -0.0500,\n",
       "                           -0.1615,     -0.0500,     -0.0740,     -0.0995,     -0.1649,\n",
       "                            0.1233,     -0.1662,      0.1812,     -0.0958,     -0.0327,\n",
       "                            0.1329,      0.0481,     -0.0565,     -0.1470,     -0.0746,\n",
       "                            0.1476,     -0.0689,      0.1548,     -0.0314,     -0.0280,\n",
       "                            0.0724,     -0.0268,     -0.0116,      0.1264,      0.0523],\n",
       "                      [     0.0268,      0.1935,      0.2529,      0.0460,      0.0224,\n",
       "                            0.1372,     -0.0160,     -0.1427,      0.0027,      0.1712,\n",
       "                            0.0553,      0.2441,     -0.0787,      0.2593,     -0.0762,\n",
       "                            0.0839,     -0.0174,     -0.0881,      0.0819,      0.0287,\n",
       "                           -0.1158,      0.0801,      0.1469,      0.1405,      0.1443,\n",
       "                           -0.1556,      0.1086,      0.0456,      0.0638,      0.0058],\n",
       "                      [     0.0175,     -0.1775,     -0.1365,      0.1489,     -0.0607,\n",
       "                           -0.0493,     -0.1156,     -0.1092,      0.1196,      0.0139,\n",
       "                           -0.0608,      0.0016,     -0.1039,     -0.2117,     -0.1813,\n",
       "                            0.0478,     -0.1153,      0.1121,      0.1560,      0.0109,\n",
       "                            0.1569,     -0.1386,      0.1470,     -0.1030,     -0.0421,\n",
       "                            0.1212,     -0.1049,      0.1393,     -0.0740,     -0.0036],\n",
       "                      [     0.1668,      0.1626,     -0.0552,      0.0177,      0.0402,\n",
       "                            0.1680,      0.1026,      0.1478,      0.1239,     -0.1379,\n",
       "                            0.0247,     -0.0153,      0.1534,      0.1365,     -0.2561,\n",
       "                           -0.0736,     -0.0001,      0.0028,     -0.1690,      0.0266,\n",
       "                            0.0666,      0.1378,      0.0855,      0.0399,     -0.1975,\n",
       "                            0.1103,     -0.1336,     -0.1785,      0.0407,     -0.0074],\n",
       "                      [     0.1009,     -0.0390,     -0.1039,     -0.0157,      0.0504,\n",
       "                           -0.1121,     -0.1586,      0.0872,     -0.1272,     -0.1135,\n",
       "                            0.1771,     -0.1420,     -0.1515,      0.1279,     -0.0289,\n",
       "                            0.0192,      0.1798,     -0.0510,      0.1701,      0.1357,\n",
       "                            0.1583,      0.0362,     -0.0761,      0.1095,     -0.1187,\n",
       "                           -0.1013,     -0.0930,      0.1049,      0.0052,      0.1392],\n",
       "                      [     0.1200,      0.0107,      0.1064,     -0.1319,     -0.1745,\n",
       "                           -0.1487,      0.1007,      0.1679,     -0.0505,      0.1010,\n",
       "                           -0.1305,     -0.0034,     -0.0011,     -0.0529,     -0.0885,\n",
       "                            0.0857,     -0.0159,     -0.0362,      0.1268,     -0.1386,\n",
       "                            0.1192,      0.1621,     -0.1122,     -0.1730,      0.0254,\n",
       "                           -0.1389,      0.0763,     -0.1456,     -0.1425,      0.0494],\n",
       "                      [    -0.0468,     -0.1616,      0.0712,      0.0645,      0.0246,\n",
       "                            0.1193,     -0.0733,      0.1666,     -0.1391,      0.1646,\n",
       "                            0.1299,     -0.1653,     -0.0162,     -0.0982,      0.1843,\n",
       "                           -0.0164,      0.0996,      0.1619,     -0.1408,      0.1110,\n",
       "                           -0.1521,     -0.0493,     -0.0694,     -0.1708,     -0.1718,\n",
       "                           -0.1746,     -0.1412,     -0.1014,     -0.0602,      0.1029],\n",
       "                      [    -0.0293,     -0.1808,     -0.1326,      0.1310,     -0.1782,\n",
       "                           -0.0898,     -0.1652,      0.0982,      0.1248,      0.0160,\n",
       "                           -0.0918,     -0.0443,      0.0106,      0.0765,      0.0032,\n",
       "                           -0.0851,      0.0088,      0.0056,      0.0017,      0.0794,\n",
       "                           -0.0688,      0.0115,      0.0008,      0.0771,     -0.1118,\n",
       "                           -0.1427,     -0.1486,      0.0767,      0.1452,      0.1811],\n",
       "                      [     0.0652,     -0.0372,      0.0336,     -0.1805,      0.0211,\n",
       "                           -0.0603,     -0.0139,      0.1049,     -0.0628,     -0.1383,\n",
       "                            0.0110,     -0.0508,     -0.0851,     -0.0558,     -0.1035,\n",
       "                            0.1603,      0.0126,     -0.0969,      0.1164,      0.0282,\n",
       "                            0.1048,      0.1407,      0.1804,     -0.0473,      0.0054,\n",
       "                           -0.1058,      0.1666,      0.0581,     -0.0302,      0.0458],\n",
       "                      [     0.1953,      0.1065,      0.1138,      0.1686,      0.0324,\n",
       "                            0.0247,      0.0453,     -0.1687,      0.0791,      0.0256,\n",
       "                           -0.1749,      0.0500,      0.1708,      0.0805,     -0.4158,\n",
       "                            0.0733,      0.0587,      0.0093,      0.0755,      0.0415,\n",
       "                           -0.1487,      0.1612,     -0.1441,     -0.0953,     -0.0406,\n",
       "                           -0.1582,      0.1798,      0.0621,      0.1393,      0.0058],\n",
       "                      [    -0.1761,      0.1603,      0.0340,     -0.1153,     -0.1259,\n",
       "                            0.0457,      0.1100,     -0.0489,     -0.0430,     -0.0333,\n",
       "                           -0.1696,      0.1354,     -0.0151,     -0.1656,     -0.1369,\n",
       "                            0.0584,     -0.1738,      0.1286,     -0.1508,     -0.0045,\n",
       "                            0.0538,     -0.0827,     -0.1183,     -0.0186,      0.0338,\n",
       "                           -0.0325,      0.0825,      0.0956,      0.0773,      0.1687],\n",
       "                      [    -0.1112,     -0.1471,      0.0442,      0.0332,      0.1749,\n",
       "                            0.1764,      0.0783,      0.1081,      0.0825,     -0.0143,\n",
       "                           -0.1711,     -0.0734,     -0.1553,      0.1290,     -0.0987,\n",
       "                           -0.1441,     -0.0243,      0.1242,      0.0798,      0.0662,\n",
       "                           -0.0111,     -0.1680,     -0.1083,     -0.1547,     -0.0547,\n",
       "                            0.1259,     -0.1213,     -0.0470,      0.1779,      0.1254],\n",
       "                      [     0.0793,      0.1560,     -0.0065,     -0.0980,      0.1185,\n",
       "                            0.0756,     -0.0566,      0.1409,     -0.1078,      0.0261,\n",
       "                            0.1631,      0.1179,     -0.1336,      0.0597,     -0.0261,\n",
       "                            0.0916,      0.1272,     -0.1292,     -0.0601,      0.1785,\n",
       "                           -0.1331,      0.0101,     -0.0588,     -0.0322,     -0.0122,\n",
       "                            0.1049,      0.1244,     -0.0654,      0.0815,     -0.1694],\n",
       "                      [    -0.0690,     -0.0171,     -0.0066,      0.0333,      0.0100,\n",
       "                           -0.0543,      0.1315,     -0.0598,      0.1458,      0.1078,\n",
       "                           -0.0040,     -0.0673,      0.1124,      0.0020,     -0.0993,\n",
       "                            0.0989,      0.0141,      0.1752,     -0.1796,      0.0445,\n",
       "                           -0.1668,     -0.0813,     -0.0180,      0.1483,     -0.0759,\n",
       "                           -0.1060,     -0.0119,     -0.0223,     -0.1447,      0.0052]])),\n",
       "             ('layers.2.bias',\n",
       "              tensor([ 0.1778, -0.0806, -0.2006, -0.1292,  0.0867, -0.0156,  0.1208,  0.0636,\n",
       "                      -0.0469,  0.1043,  0.0702, -0.1292,  0.0308, -0.0052, -0.1201,  0.0705,\n",
       "                       0.0852, -0.0775, -0.1366,  0.0773])),\n",
       "             ('layers.4.weight',\n",
       "              tensor([[-0.5684,  0.0494, -0.0255,  0.0374,  0.0446,  0.3846, -0.2262,  0.1745,\n",
       "                       -0.1032,  0.2297, -0.0885,  0.1948,  0.0320, -0.0398, -0.0949,  0.2999,\n",
       "                       -0.1419, -0.1026,  0.0244, -0.1448],\n",
       "                      [ 0.5507,  0.0563, -0.2069, -0.2204,  0.0975, -0.4139, -0.0619, -0.3958,\n",
       "                        0.0549, -0.1653,  0.1045, -0.2146,  0.2562, -0.1905,  0.0303, -0.3036,\n",
       "                       -0.1604, -0.0063, -0.3451,  0.1861]])),\n",
       "             ('layers.4.bias', tensor([-0.1498,  0.1599]))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:27:04.926146Z",
     "start_time": "2024-12-13T11:27:04.921593Z"
    }
   },
   "cell_type": "code",
   "source": "model.state_dict().keys()",
   "id": "3ff023cb3d1950ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['layers.0.weight', 'layers.0.bias', 'layers.2.weight', 'layers.2.bias', 'layers.4.weight', 'layers.4.bias'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:45:23.701394Z",
     "start_time": "2024-12-13T11:45:23.682065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "def git_codebase_root():\n",
    "    try:\n",
    "        root = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"--show-toplevel\"], stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        return Path(root.decode().strip())\n",
    "    except subprocess.CalledProcessError:\n",
    "        logger.warning(\"Not inside a Git repository.\")\n",
    "        return None\n",
    "\n",
    "def get_working_directory_or_git_root():\n",
    "    git_root = git_codebase_root()\n",
    "    return git_root if git_root is not None else Path.cwd()\n",
    "\n",
    "root_dir = get_working_directory_or_git_root()\n",
    "#print(root_dir)"
   ],
   "id": "2960cfc57327b1f4",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:46:14.510289Z",
     "start_time": "2024-12-13T11:46:14.505571Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), root_dir / \"models/NN2x2.pth\")",
   "id": "304ed8f313d724c8",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The model's `state_dict` is a Python dictionary object that maps each layer in the model to its trainable parameters (weights and biases).\n",
    "\n",
    "`model.pth` is an arbitrary filename for the model file saved to disk. We can give it any name and file ending we like; however, `.pth` and `.pt` are the most common conventions."
   ],
   "id": "10fca94412f14cd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once we saved the model, we can restore it from disk:",
   "id": "9ceab1a56f06a94b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:51:28.612784Z",
     "start_time": "2024-12-13T11:51:28.602298Z"
    }
   },
   "cell_type": "code",
   "source": "model_restored = NeuralNetwork(2,2) # This line is not strictly necessary if you execute this code in the same session where you saved a model. However, I included it here to illustrate that we need an instance of the model in memory to apply the saved parameters. Here, the NeuralNetwork(2, 2) architecture needs to match the original saved model exactly.",
   "id": "e2ff41d9d3f4aa70",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:53:14.083922Z",
     "start_time": "2024-12-13T11:53:14.076989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_restored.load_state_dict(torch.load(root_dir / \"models/NN2x2.pth\"))\n",
    "# The `torch.load(root_dir / \"models/NN2x2.pth\")` function reads the file \"NN2x2.pth\" and reconstructs the Python dictionary object containing the model’s parameters while model.load_state_dict() applies these parameters to the model, effectively restoring its learned state from when we saved it."
   ],
   "id": "a7f7d49668b62a66",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mb/dzmlylm1041255njxgdgpt4m0000gn/T/ipykernel_17952/3233428668.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_restored.load_state_dict(torch.load(root_dir / \"models/NN2x2.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:54:08.636556Z",
     "start_time": "2024-12-13T11:54:08.632657Z"
    }
   },
   "cell_type": "code",
   "source": "model_restored",
   "id": "c6fb17d430a6e848",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
